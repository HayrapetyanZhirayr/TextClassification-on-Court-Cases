{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = os.path.expanduser('~')\n",
    "tar_archive_path = os.path.join(os.curdir, 'raw_data',\n",
    "                                '33m-russian-courts-cases-by-suvorov', \n",
    "                                'arb_sud.tar'\n",
    "#                                 'project2.tar'\n",
    "                               )\n",
    "\n",
    "arb_sud_folder = os.path.join(\n",
    "    os.curdir,\n",
    "    'raw_data',\n",
    "    '33m-russian-courts-cases-by-suvorov',\n",
    "    'arb_sud'\n",
    ")\n",
    "\n",
    "import tarfile\n",
    "tar_archive = tarfile.open(tar_archive_path, 'r')\n",
    "import pandas as pd\n",
    "import glob\n",
    "import zlib\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display\n",
    "import utils\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "from importlib import reload\n",
    "n_files = 14810696  # number of files in arb_sud.tar archive\n",
    "reload(utils);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:Papyrus; font-size:1.5em;\">This is an auxiliray notebook for data format manipulations </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_to_category_dict = utils.get_case_to_category_mapping()\n",
    "# pd.read_csv('разметка/2007_match_first.csv').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(archive, doc_link):\n",
    "        doc_bin_gz =  archive.extractfile(doc_link).read()\n",
    "        doc_bin_xml = zlib.decompress(doc_bin_gz, 16+zlib.MAX_WBITS)\n",
    "        soup = BeautifulSoup(doc_bin_xml, 'lxml')\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading torrent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solutions_info = json.load(open('solutions_info.json', 'r'))\n",
    "# progress_counter = json.load(open('progress_counter.json', 'r'))[0]\n",
    "# regions_parsed = set(json.load(open('regions_parsed.json', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pbar = tqdm(total=n_files)\n",
    "# pbar.update(progress_counter)\n",
    "# for region_folder in all_entries_in(arb_sud_folder):\n",
    "#     if region_folder not in regions_parsed:\n",
    "#         for year_folder in all_entries_in(region_folder):\n",
    "#             for doc_xml_gz_path in all_entries_in(year_folder):\n",
    "#                 doc_xml_gz = gzip.open(doc_xml_gz_path)\n",
    "#                 s = BeautifulSoup(doc_xml_gz.read())\n",
    "#                 doc_xml_gz.close()\n",
    "#                 doc_tags = [\n",
    "#                             text), s.region, s.court, s.vidpr, \n",
    "#                             s.etapd, s.category, s.date, s.vid_dokumenta\n",
    "#                         ]\n",
    "#                 if doc_tags[0] is None:\n",
    "#                     doc_tags[0] = s.case_number\n",
    "#                 doc_info = [tag.text if tag else None for tag in doc_tags]\n",
    "#                 solutions_info.append(doc_info)\n",
    "#                 pbar.update(1)\n",
    "#                 progress_counter += 1\n",
    "#         regions_parsed.add(region_folder)\n",
    "\n",
    "#         with open('solutions_info.json', 'w') as f:\n",
    "#             json.dump(solutions_info, f)\n",
    "\n",
    "#         with open('progress_counter.json', 'w') as f:\n",
    "#             json.dump([progress_counter], f)\n",
    "\n",
    "#         with open('regions_parsed.json', 'w') as f:\n",
    "#             json.dump(list(regions_parsed), f)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solutions_info_df = pd.read_csv('data/t_solutions_info_df.csv', index_col=0, dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to map some category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # category_name_map = {}\n",
    "# # not_regognized = set()\n",
    "# for cat_in in solutions_info_df['category'].dropna().unique():\n",
    "#     if cat_in in category_name_map or cat_in in not_regognized:\n",
    "#         continue\n",
    "#     print('ello')\n",
    "#     distances = []\n",
    "#     for _, cat_out_obj in categs_info_df.drop(categs_info_df.tail(1).index)[['сCode', 'Descr']].iterrows():\n",
    "#         cat_out_idx = cat_out_obj['сCode']\n",
    "#         cat_out = cat_out_obj['Descr']\n",
    "#         d = Levenshtein.distance(cat_in, cat_out)\n",
    "#         distances.append((cat_out_idx, cat_out, d))\n",
    "#     distances = sorted(distances, key=lambda x: x[-1])\n",
    "#     d_min = distances[0][-1]\n",
    "#     if d_min < 45:\n",
    "#         n = 3\n",
    "#         candidate_distances = distances[:n]\n",
    "#         print(f'Input category :: {cat_in}')\n",
    "#         for i, (cat_out_idx, cat_out, d) in enumerate(candidate_distances, 1):\n",
    "#             print(f'\\t{i}. {cat_out_idx} {cat_out}. d :: {d}')\n",
    "#         input_deсision = int(input('Input 0 for None: '))\n",
    "#         if input_deсision in range(1, n+1):\n",
    "#             decision_cat_out = candidate_distances[input_deсision-1][1]\n",
    "#             category_name_map[cat_in] = decision_cat_out\n",
    "#         else:\n",
    "#             not_regognized.add(cat_in)\n",
    "#         display.clear_output()\n",
    "#     else:\n",
    "#         not_regognized.add(cat_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/not_recognized.json', 'w') as f:\n",
    "    json.dump(list(not_regognized), f, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoKad -> Standart Form (Saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\n",
    "    os.path.join(\n",
    "#         os.path.expanduser('~'), \n",
    "        'kad.arbitr-parser/data.json',\n",
    "    ), 'r'\n",
    "))\n",
    "reload(utils);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 0\n",
    "for cat in data:\n",
    "    z += len(data[cat])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.mkdir('raw_data/autoKad_data')\n",
    "\n",
    "root_dir = os.path.join(\n",
    "    os.curdir,\n",
    "    'raw_data',\n",
    "    'autoKad_data'\n",
    ")\n",
    "\n",
    "for category in data:\n",
    "    for document in data[category]:\n",
    "        date, court, text = document\n",
    "        # FINDING CASENUMBER FROM TEXT =====\n",
    "        casenumber_raw, cn_is_extracted = utils.extract_casenumber(doc=text)\n",
    "        casenumber, cn_is_processed = utils.process_raw_casenumber(casenumber_raw)\n",
    "        # ===================================\n",
    "        year = date.split('.')[-1]\n",
    "        court_dir = os.path.join(root_dir, court)\n",
    "        year_dir = os.path.join(court_dir, year)\n",
    "        utils.mkdir(court_dir)\n",
    "        utils.mkdir(year_dir)\n",
    "        file_path = os.path.join(year_dir,\n",
    "            hashlib.sha1(' '.join(document + [category]).encode('utf-8')).hexdigest() + '.html.gz'\n",
    "        )\n",
    "        doc_html_string = utils.make_xml_string(*document, casenumber, category)\n",
    "        doc_html_bytes = bytes(doc_html_string, 'utf-8')\n",
    "        f_out = gzip.open(file_path, 'wb')\n",
    "        f_out.write(doc_html_bytes)\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating catID's in cases df in agreement with taxonomy_df\n",
    "### Building cases_df for autoKad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whoisjiji/.local/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "cases_df_old = pd.read_csv('temp_data/cases_info_old.csv', index_col=0)\n",
    "old_categs_df = pd.read_csv('temp_data/categs_info_old.csv', index_col=0)\n",
    "taxonomy_df = pd.read_csv('data/taxonomies/taxonomy_df.csv')\n",
    "oldID2newID = json.load(open('temp_data/oldID2newID_dict.json', 'r'), \n",
    "                        object_hook = (lambda d: {int(k):int(v) for k, v in d.items()})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df_new = cases_df_old.copy()\n",
    "cases_df_new['CategoryID'] = cases_df_old['CategoryID'].map(\n",
    "                                lambda x: oldID2newID.get(x, -1)\n",
    ")\n",
    "cases_df_new.to_csv('data/cases_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kada_data = []\n",
    "read_dir = 'raw_data/autoKad_data/'\n",
    "for doc in utils.yield_document(read_dir):\n",
    "    s = BeautifulSoup(doc, 'html.parser')\n",
    "    doc_tags = [\n",
    "        s.court, s.casenumber, s.category, s.date,\n",
    "        ]\n",
    "    doc_info = [tag.text if tag else None for tag in doc_tags]\n",
    "    court, casenumber, category, date = doc_info\n",
    "    \n",
    "    # we care only about docs with casenumber and category from taxonomy_df\n",
    "    if (casenumber is not None) and (category is not None) and (not utils.date_is_anomal(date)):\n",
    "        date = utils.change_date_format(date)\n",
    "        cat_index, *cat_name_list = category.split('. ')\n",
    "        cat_name = '. '.join(cat_name_list)\n",
    "        cCode_mask = taxonomy_df['cCode'] == cat_index\n",
    "        cat_is_in_taxonomy = cCode_mask.sum() == 1\n",
    "        if cat_is_in_taxonomy:\n",
    "            ID = taxonomy_df[cCode_mask]['ID'].values[0]\n",
    "            kada_data.append([casenumber, ID, -1, date, date])\n",
    "kada_cases_df = pd.DataFrame(kada_data, columns=cases_df_new.columns)\n",
    "kada_cases_df.to_csv('temp_data/kada_cases_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making 2 stratified samples (large and smalll) of Suvorov Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = set(json.load(open('data/regions.json', 'r')))\n",
    "courts = set(json.load(open('data/courts.json', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LARGE = 10**6\n",
    "N_SMALL = 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_info_df = pd.read_csv('data/t_solutions_info_df.csv', index_col=0, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mask = solutions_info_df['region'].isin(regions)\n",
    "court_mask =  solutions_info_df['court'].isin(courts)\n",
    "first_inst_mask = solutions_info_df['etapd'] == 'Первая инстанция'\n",
    "\n",
    "labeled_mask = solutions_info_df['casenumber'].isin(case_to_category_dict)\n",
    "\n",
    "overall_mask = (\n",
    "    labeled_mask &\n",
    "    region_mask &\n",
    "    court_mask &\n",
    "    first_inst_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = solutions_info_df[overall_mask]\n",
    "df = df.assign(\n",
    "    label = lambda x: x['casenumber'].apply(lambda x: case_to_category_dict[x]).values, \n",
    ")\n",
    "df = df.assign(\n",
    "    strata = lambda x: x['court'] + '.' +  x['region'] + '.' + x['label'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sample_df = (\n",
    "    df.groupby('strata', group_keys=False)\n",
    "        .apply(lambda x: x.sample(frac=N_LARGE/len(df), random_state=1))\n",
    ")\n",
    "\n",
    "small_sample_df = (\n",
    "    df.groupby('strata', group_keys=False)\n",
    "        .apply(lambda x: x.sample(frac=N_SMALL/len(df), random_state=1))\n",
    ")\n",
    "\n",
    "large_sample_df.to_csv('data/t_solutions_info_df_large_sample.csv') \n",
    "small_sample_df.to_csv('data/t_solutions_info_df_small_sample.csv')\n",
    "\n",
    "large_case_numbers = set(large_sample_df['casenumber'].unique())\n",
    "small_case_numbers = set(small_sample_df['casenumber'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir('data/cases_large')\n",
    "# mkdir('data/cases_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_large_folder = os.path.join(os.curdir, 'data', 'cases_large')\n",
    "cases_small_folder = os.path.join(os.curdir, 'data', 'cases_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file_to_case_folder(source, cases_folder):\n",
    "    residual, source_name = os.path.split(source)\n",
    "    residual, year = os.path.split(residual)\n",
    "    residual, region = os.path.split(residual)\n",
    "\n",
    "    region_folder = os.path.join(cases_folder, region)\n",
    "    mkdir(region_folder)\n",
    "    year_folder = os.path.join(region_folder, year)\n",
    "    mkdir(year_folder)\n",
    "\n",
    "    destination = os.path.join(year_folder, source_name)\n",
    "\n",
    "    shutil.copyfile(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63ad8a214ad4680a185ddcfa9d3f527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14810696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(total=n_files)\n",
    "pbar.update(progress_counter)\n",
    "cases_large_folder = os.path.join(os.curdir, 'data', 'cases_large')\n",
    "cases_small_folder = os.path.join(os.curdir, 'data', 'cases_small')\n",
    "for region_folder in all_entries_in(arb_sud_folder):\n",
    "    for year_folder in all_entries_in(region_folder):\n",
    "        for doc_xml_gz_path in all_entries_in(year_folder):\n",
    "            doc_xml_gz = gzip.open(doc_xml_gz_path)\n",
    "            s = BeautifulSoup(doc_xml_gz.read())\n",
    "            doc_xml_gz.close()\n",
    "            casenumber = s.casenumber\n",
    "            if casenumber is None:\n",
    "                casenumber = s.case_number\n",
    "            if casenumber:\n",
    "                casenumber = casenumber.text\n",
    "\n",
    "            if casenumber in large_case_numbers:\n",
    "                copy_file_to_case_folder(doc_xml_gz_path, cases_large_folder)\n",
    "            if casenumber in small_case_numbers:\n",
    "                copy_file_to_case_folder(doc_xml_gz_path, cases_small_folder)\n",
    "                \n",
    "            pbar.update(1)\n",
    "            progress_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress_counter=0\n",
    "pbar = tqdm(total=n_files)\n",
    "for region_folder in all_entries_in(arb_sud_folder):\n",
    "    for year_folder in all_entries_in(region_folder):\n",
    "        for doc_xml_gz_path in all_entries_in(year_folder):\n",
    "            doc_xml_gz = gzip.open(doc_xml_gz_path)\n",
    "            doc = doc_xml_gz.read().decode('utf8')\n",
    "            s = BeautifulSoup(doc, 'html.parser')\n",
    "            \n",
    "            pbar.update(1)\n",
    "            progress_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING CATEGS MAP DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CATEGS INFO ##########################################\n",
    "categs_info = pd.read_csv('temp_data/categs_info_old.csv')\n",
    "cCode2descr = dict(zip(\n",
    "                categs_info['cCode'].str.strip(), \n",
    "                categs_info['Descr'].str.strip()\n",
    "))\n",
    "cCode2descr.pop('Нет', None)\n",
    "descr2cCode = {val:key for key, val in cCode2descr.items()}\n",
    "############################################################\n",
    "\n",
    "### CREATING categs mapping df #############################\n",
    "rcg_dict = json.load(open('temp_data/category_name_map.json'))\n",
    "not_rcg_list = json.load(open('temp_data/not_recognized.json'))\n",
    "not_rcg_dict = {name:'' for name in not_rcg_list}\n",
    "\n",
    "categs_name_df = pd.DataFrame(\n",
    "    list(rcg_dict.items()) + \n",
    "    list(not_rcg_dict.items())\n",
    ")\n",
    "categs_name_df.rename(columns={0:'torent', 1:'kadarbitr'}, inplace=True)\n",
    "categs_name_df['kadarbitr'] = (\n",
    "    categs_name_df['kadarbitr'].map(lambda descr: descr2cCode.get(descr, '')) + \n",
    "    '. ' \n",
    "    + categs_name_df['kadarbitr']\n",
    ")  # adding index to names\n",
    "categs_name_df['kadarbitr'][categs_name_df['kadarbitr'] == '. '] = ''\n",
    "categs_name_df.to_csv('data/categs_map_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING AUTOKAD DATA v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "read_dir = 'raw_data/autoKad_data/'\n",
    "save_dir = 'data/autoKad_data_v1/'\n",
    "utils.mkdir(save_dir)\n",
    "z = []\n",
    "for doc_path, doc in utils.yield_document2(read_dir):\n",
    "    court, casenumber, category, date, text  = utils.process_xml_string(doc)\n",
    "    z.append(p_doc_path)\n",
    "    p_text_list = utils.preprocess_text(text, lemmatize=True)\n",
    "    p_text = ' '.join(p_text_list)\n",
    "    \n",
    "    #### extracting YEAR and REG and NAME from PATH ########\n",
    "    year_folder, name = os.path.split(doc_path)\n",
    "    region_folder, year = os.path.split(year_folder)\n",
    "    _, region = os.path.split(region_folder)\n",
    "    ###############################################\n",
    "    p_region_folder = os.path.join(save_dir, region)\n",
    "    utils.mkdir(p_region_folder)\n",
    "    p_year_folder = os.path.join(p_region_folder, year)\n",
    "    utils.mkdir(p_year_folder)\n",
    "    p_doc_path = os.path.join(p_year_folder, name)\n",
    "    ###############################################\n",
    "    doc_html_string = utils.make_xml_string(date, court, p_text, casenumber, category)\n",
    "    doc_html_bytes = bytes(doc_html_string, 'utf-8')\n",
    "    f_out = gzip.open(p_doc_path, 'wb')\n",
    "    f_out.write(doc_html_bytes)\n",
    "    f_out.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
